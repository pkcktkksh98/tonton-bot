# Local LLM Streamlit FAQ Assistant

TontonBot is a lightweight FAQ assistant built using a locally hosted LLM model (`Nous-Hermes-2-Mistral-7B-DPO`) and a Streamlit frontend interface. It leverages HuggingFace's Transformers and supports context-based generation for Tonton-like services.

---

## âœ… Features

- Local LLM inference using Transformers (no internet needed after setup)
- Short or long responses based on context (not forced to use max tokens)
- Simple Streamlit UI for input/output
- Modular code structure (easy to extend with RAG, embeddings, etc.)

---

## ğŸ“ Project Structure

```
lexibot_llm_streamlit/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ interface.py          # Streamlit frontend
â”‚   â””â”€â”€ generator.py          # Model loading and answer generation
â”‚                 
â”œâ”€â”€ data/                     # Placeholder for data context or FAISS/RAG
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
---

## ğŸ§± System Architecture
```
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚       Offline Preprocess     â”‚
                                        â”‚  (generate FAISS embeddings) â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚     1. Generate FAISS Vector Index   â”‚
                                     â”‚     - Using sentence-transformers    â”‚
                                     â”‚     - distiluse-base-multilingual    â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚           2. Run Streamlit App             â”‚
                                  â”‚  - Loads FAISS index and metadata          â”‚
                                  â”‚  - Loads HuggingFace model:                â”‚
                                  â”‚    Nous-Hermes-2-Mistral-7B-DPO            â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                               User Query Input
                                                       â”‚
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚    Semantic Embedding       â”‚
                                          â”‚ (same embedding model used) â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚    FAISS Vector Search     â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚ HuggingFace Transformer LLM â”‚
                                          â”‚  (Nous-Hermes-2-Mistral-7B) â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                Final Response
```
---

## ğŸš€ Getting Started

1. **Create Environment**:
   ```bash
   conda create -n tontonbot python=3.11
   conda activate tontonbot
   cd tontonbot
   ```

2. **Install Requirements**:
   ```bash
   pip install -r requirements.txt
   ```
   
3. **Generate FAISS Vector**:
   ```bash
   python app/ingest.py
   ```
   
## ğŸ³ Docker Setup

You can containerize and run the Tonton FAQ Bot using Docker with the following steps:

### ğŸ“¦ 1. Build the Docker Image

Make sure you're in the root project directory (same level as the `Dockerfile`), then run:

```bash
docker build -t tonton-bot .
```

### â–¶ï¸ 2. Run the Container

```bash
docker run -p 8501:8501 tonton-bot
```

Once running, open your browser and go to:

```
http://localhost:8501
```

---

## ğŸ“¦ Requirements

```
langchain
faiss-cpu
transformers
sentence-transformers
streamlit
torch
PyMuPDF
llama-cpp-python
tiktoken
sentencepiece
llama-index
langchain-community
```

(Optional: `accelerate`, `sentencepiece` if required by model)

---

# ğŸ“‹ Example Questions That Can Be Asked

- Kenapa lepas bayar aku tak boleh tengok cerita?
- Aku dah langgan, tapi kenapa masih keluar iklan?
- Macam mana nak batal langganan TontonUp setiap bulan?
- Macam mana nak tukar password?
- Boleh tak aku langgan kalau aku kat luar Malaysia?
- Kenapa keluar error masa aku tengok Tonton?
  
---

## ğŸ“Œ Notes

- This project assumes the model is downloaded from HuggingFace or accessible locally.
   
---    

## ğŸ›¡ï¸ License

MIT License. Feel free to use and modify.
